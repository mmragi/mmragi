<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
  <meta content="width=device-width, initial-scale=1.0" name="viewport">

  <title>2nd Workshop on Multi-Modal Reasoning for Agentic Intelligence</title>
  <meta content="2nd Workshop on Multi-Modal Reasoning for Agentic Intelligence" name="description">
  <meta content name="keywords">

  <!-- Favicons -->
  <link href="assets/img/cvpr.png" rel="icon">
  <link href="assets/img/apple-touch-icon.png" rel="apple-touch-icon">

  <!-- Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i, Raleway:300,300i,400,400i,500,500i,600,600i,700,700i, Poppins:300,300i,400,400i,500,500i,600,600i,700,700i"
    rel="stylesheet">

  <!-- Vendor CSS Files -->
  <link href="assets/vendor/animate.css/animate.min.css" rel="stylesheet">
  <link href="assets/vendor/aos/aos.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
  <link href="assets/vendor/bootstrap-icons/bootstrap-icons.css" rel="stylesheet">
  <link href="assets/vendor/boxicons/css/boxicons.min.css" rel="stylesheet">
  <link href="assets/vendor/glightbox/css/glightbox.min.css" rel="stylesheet">
  <link href="assets/vendor/remixicon/remixicon.css" rel="stylesheet">
  <link href="assets/vendor/swiper/swiper-bundle.min.css" rel="stylesheet">

  <!-- Template Main CSS File -->
  <link href="assets/css/style.css" rel="stylesheet">

  <style>
    li {
      margin: 10px 10px 10px 10px;
    }
  </style>

  <style>
    .interactive-panel {
      padding: 10px;
      border: 1px solid #ddd;
      border-radius: 5px;
      transition: all 0.3s ease;
      margin-bottom: 15px;
      /* Add margin to the bottom of each panel */
    }

    .interactive-panel:hover {
      border-color: #007bff;
      box-shadow: 0 0 10px rgba(0, 123, 255, 0.5);
      cursor: pointer;
    }
  </style>

  <style>
    /* CSS for coloring schedule rows */
    .morning-session {
      background-color: #fff4d5;
      /* Light yellow for morning sessions */
    }

    .afternoon-session {
      background-color: #dbe7f2;
      /* Light blue for afternoon sessions */
    }

    .evening-session {
      background-color: #e9daeb;
      /* Light purple for evening sessions */
    }

    .night-session {
      background-color: #f9f0ed;
      /* Light purple for evening sessions */
    }
  </style>

</head>

<body>

  <!-- ======= Header ======= -->
  <header id="header" class="fixed-top d-flex align-items-center  header-transparent ">
    <div class="container d-flex align-items-center justify-content-between">

      <div class="logo">
        <a href="#hero">
          <img src="assets/img/cvpr-navbar-logo.svg" alt="CVPR" style="height: 40px; width: auto;">
        </a>
        <!-- <h1><a href="https://llmagents.github.io/" target="blank_"> </a></h1> -->
        <!-- <img src="assets/img/logo.png" alt="TamingLLM@SIGDIAL & INLG 2023"> -->

        <!-- <p style="margin : 0; padding-top:0; padding-left: 80px; padding-bottom:0;  line-height:0; font-size: 10px; text-align: center;" class="green-text">
        May 26-28, 2022 ,  Dublin
      </p> -->
      </div>

      <nav id="navbar" class="navbar">
        <ul>
          <li><a class="nav-link scrollto active" href="#hero">Home</a></li>
          <!-- <li><a class="nav-link scrollto" href="#about">About</a></li> -->
          <li><a class="nav-link scrollto" href="#Callforpapers">Call for Papers</a></li>
          <!-- <li><a class="nav-link scrollto" href="#topics">Topics</a></li> -->
          <!-- <li><a class="nav-link scrollto" href="#cfp">Call for
              Papers</a></li>
          <li><a class="nav-link scrollto" href="#accepted-papers">Accepted
              Papers</a></li> -->
          <li><a class="nav-link scrollto" href="#speaker">Speakers</a></li>
          <!-- <li><a class="nav-link scrollto" href="#panelist">Panelists</a></li> -->

          <li><a class="nav-link scrollto" href="#schedule">Schedule</a></li>
          <li><a class="nav-link scrollto" href="#org">Organization</a></li>
          <!-- <li><a class="nav-link scrollto" href="#reviewers">Reviewers</a></li> -->
          <li><a class="nav-link scrollto" href="#contact">Contact</a></li>
        </ul>
        <i class="bi bi-list mobile-nav-toggle"></i>
      </nav><!-- .navbar -->

    </div>
  </header><!-- End Header -->

  <!-- ======= Hero Section ======= -->

  <section id="hero" class="d-flex flex-column justify-content-end align-items-center">
    <div id="heroCarousel" data-bs-interval="5000" class="container carousel carousel-fade" data-bs-ride="carousel">

      <!-- Slide 1 -->
      <div class="carousel-item active">
        <div class="carousel-container">
          <div style="text-align: center;">
            <h2 style="font-size: 60px; color: #daa520;" class="animate__animated animate__fadeInDown">2nd Workshop on MMRAgI at CVPR 2026
            </h2>
            <h2 style="font-size: 30px; color: #6d99f7;" class="animate__animated animate__fadeInDown">Multi-Modal Reasoning for Agentic Intelligence</h2>
            <!-- <h3 style="font-size: 20px; color: #6d99f7;" class="animate__animated animate__fadeInDown">8:00A.M. - 6:00P.M. OCT 20, 2025, 301 A</h3> -->
            <h3 style="font-size: 20px; color: #6d99f7;" class="animate__animated animate__fadeInDown">June 3/4, 2026 (tentative), Denver Colorado</h3>
            <!-- <h3 style="font-size: 20px; color: #f76d6d;" class="animate__animated animate__fadeInDown">IMPORTANT: You can find our posters in Exhibit Hall II (numbers 77‚Äì84).</h3> -->
            <!-- <h2 style="font-size: 30px; color: #ffffff;" class="animate__animated animate__fadeInDown">Hawaii Convention
              Center</h2>
            <h3 style="font-size: 30px; color: #4174bb; margin-top: 0px;" class="animate__animated animate__fadeInDown">
              Honolulu, HI, USA</h3>
            <h3 style="font-size: 30px; color: #44cbd6; margin-top: 0px;" class="animate__animated animate__fadeInDown">
              October, 2025</h3> -->
          </div>
          <p class="animate__animated animate__fadeInDown">
            <!-- <a href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/MMR" target="_blank"
              style="color: lightgrey;">Proceeding Track</a> | -->
            <a href="https://openreview.net/group?id=thecvf.com/CVPR/2026/Workshop/MMRAgI" target="_blank"
              style="color: lightgrey;">Submit Here</a> |
            <a href="https://cvpr.thecvf.com/" target="_blank" style="color: lightgrey;">CVPR 2026</a>
          </p>
          

          <p class="animate__animated fanimate__adeInUp">

          </p>
        </div>
      </div>

    </div>

    <svg class="hero-waves" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
      viewBox="0 24 150 28 " preserveAspectRatio="none">
      <defs>
        <path id="wave-path" d="M-160 44c30 0 58-18 88-18s 58 18 88 18 58-18 88-18 58 18 88 18 v44h-352z">
      </defs>
      <g class="wave1">
        <use xlink:href="#wave-path" x="50" y="3" fill="rgba(255,255,255, .1)">
      </g>
      <g class="wave2">
        <use xlink:href="#wave-path" x="50" y="0" fill="rgba(255,255,255, .2)">
      </g>
      <g class="wave3">
        <use xlink:href="#wave-path" x="50" y="9" fill="#fff">
      </g>
    </svg>

  </section><!-- End Hero -->

  <main id="main">
    <section id="about" class="contact">
      <div class="container">
        <div class="row">
          <div class="col-md-1">
          </div>
          <div class="col-md-9">
            <p style="font-size: 20px; margin-bottom: 20px; text-align: justify;">
              <strong>Motivation.</strong> Artificial Intelligence (AI) agents powered by Large Language Models (LLMs) have demonstrated advanced capabilities in automating complex tasks. With the rapid evolution of LLMs‚Äô reasoning abilities, such agents have achieved notable success in a variety of domains, including software development and robotics. More recently, the emergence of Multimodal Foundation Models (MFMs) marks a significant advancement in artificial intelligence. These models integrate multiple sensory modalities‚Äîsuch as vision, language, and audio‚Äîto substantially enhance the cognitive and perceptual capabilities of AI agents.
              Recent breakthroughs reveal that MFMs enable fine-grained cross-modal reasoning by jointly processing visual, textual, and auditory data. These systems excel at parsing complex scenarios, generating context-aware descriptions, and tackling tasks requiring synergistic perception and language understanding‚Äîcapabilities critical to robotics, human-computer interaction, and beyond. Thus, this workshop aims to explore how AI agents can be further empowered through MFMs to operate effectively in multimodal environments.
            </p>
            <p style="font-size: 20px; margin-bottom: 20px; text-align: justify;">
              <strong>Background & Application.</strong> Recent advances have demonstrated the potential of 
              AI agents to operate in increasingly complex, multimodal settings. 
              For instance, OS-Copilot exhibits human-like interaction capabilities 
              with operating systems, encompassing activities such as web browsing, programming, 
              and engaging with third-party applications. Another prominent application area involves 
              AI Scientist Agents, designed to automate research workflows‚Äîincluding experiment design, 
              execution, and analysis. However, existing 
              implementations of such agents are often limited to processing textual inputs, thereby overlooking the rich, complementary information available from visual and auditory modalities. Lastly, Embodied Agents represent another rapidly evolving area, where AI agents interact with the physical (or simulated) world using sensors and robotic effectors. These applications exemplify the growing need for agents that can reason across modalities to achieve human-level situational awareness and actionability.
            <p style="font-size: 20px; margin-bottom: 20px; text-align: justify;">
              <strong>Challenges.</strong>
              Multimodal reasoning introduces a distinct set of theoretical and technical challenges that extend beyond those faced in unimodal contexts. First, integrating heterogeneous data (text, images, video) demands innovations in model architectures, training paradigms, and evaluation frameworks. The increased computational demands associated with processing multimodal inputs also raise concerns regarding scalability, efficiency, and real-time performance.
            </p>
            <p style="font-size: 20px; margin-bottom: 20px; text-align: justify;">
              Second, applying these models to frontier domains‚Äîsuch as multi-agent collaboration, scientific discovery, and
              embodied intelligence systems‚Äîrequires overcoming bottlenecks in cross-modal semantic understanding and knowledge
              transfer. Crucially, the interpretability and robustness of multimodal reasoning systems remain unresolved
              foundational issues, directly impacting the deployment of reliable real-world applications (e.g., scientific AI
              agents, bio-inspired robotics). Addressing these challenges necessitates breakthroughs in cross-modal representation
              alignment, dynamic attention mechanisms, and uncertainty modeling of multi-source information.
              </p>
              <p style="font-size: 20px; margin-bottom: 20px; text-align: justify;">
                <strong>About this Workshop.</strong> All discussions under the scope of multimodal reasoning are welcome. This
                workshop aims to bring. together researchers with various backgrounds to study the next generation of multimodal
                reasoning systems. To foster an inclusive dialogue and debate space, we invite speakers and panelists from diverse
                backgrounds and areas of expertise. Our roster includes both renowned researchers and emerging investigators who have
                driven promising advances in the field.
              
              </p>
          </div>
          <div class="col-md-1">
          </div>
        </div>
      </div>
      
      </section><!-- End About Section -->
      <!-- ======= Call for Papers Section ======= -->
      <section id="Callforpapers" class="team">
        <div class="container">
          <div class="section-title" data-aos="zoom-out">
            <h2>Call for Papers</h2>
          </div>
          <div class="container">
            <div class="row">
              <p>
                This workshop primarily focuses on the advancement of MFM-based Agents from the perspective of enhanced
                multimodal perception and reasoning. To foster an inclusive environment for discussion and debate, we welcome
                speakers and panelists from diverse backgrounds and expertise. Our lineup features distinguished researchers
                alongside emerging investigators who have made significant contributions to the field. Spotlight and poster
                sessions will highlight new ideas, key challenges, and retrospective insights related to the workshop‚Äôs
                themes.
              </p>
              <p>Relevant topics include, but are not limited to:</p>
            </div>
      
            <ul style="list-style-type: disc; padding-left: 20px; font-size: 1.05em; line-height: 1.8;">
              <li>How can we achieve semantically consistent alignment across vision, language, and audio modalities?</li>
              <li>What novel training paradigms can mitigate the computational burden of multimodal systems?</li>
              <li>What novel model architecture are effective and native modelmodal reasoners?</li>
              <li>How do we quantify and enhance the causal reasoning capabilities of multimodal foundation models?</li>
              <li>Can we develop unified metrics for evaluating cross-modal reasoning in open-ended scenarios?</li>
              <li>How can we incentivize reasoning native in multimodal representations?</li>
              <li>How to balance the reliance on different multimodal signals for inference and reasoning,</li>
              <li>How to reduce the computational demands introduced by highly redundant modalities, such as images and videos.</li>
              <li>How the multimodal signal influences the behavior of the LMM Agent.</li>
            </ul>
      
            <div class="row" style="padding: 20px; border-radius: 8px;">
              <h4 style="font-weight: bold; margin-bottom: 15px;">Submission:</h4>
              <!-- <p><strong>Submission Platform:</strong> <a
                  href="https://openreview.net/group?id=thecvf.com/ICCV/2025/Workshop/MMR" target="_blank"
                  style="color: rgb(13, 26, 218);">OpenReview</a></p> -->
      
              <p><strong>Submission:</strong> <a href="https://openreview.net/group?id=thecvf.com/CVPR/2026/Workshop/MMRAgI" target="_blank"
                style="color: rgb(13, 26, 218);">Submit to OpenReview</a></p>
              <p><strong>Submission Guideline:</strong></p>
              <ul>
                <li>Paper Formatting: Papers are limited to eight pages, including figures and tables (reference not included), in the CVPR style.</li>
                <li>Double Blind Review: CVPR reviewing is double blind, in that authors do not know the names of the area
                  chairs or reviewers for their papers, and the area chairs/reviewers cannot, beyond a reasonable doubt, infer
                  the names of the authors from the submission and the additional material.</li>
              </ul>
              <p><strong>Archival Policy: </strong>This workshop is non-archival and will not result in proceedings; workshop submissions can be submitted to other venues.</p>
              <p><strong>Dual Submission: </strong>We welcome papers that may have been already accepted at CVPR 2026 but which are also relevant to this workshop, or papers under reviews at other venues (e.g., ICML 2026).</p>
              <p><strong>Key Dates:</strong></p>
              <ul>
                <li>Paper Submission Open: February 1th 2026, 23:59 AoE Time</li>
                <li>Paper Submission Deadline: March 9th 2026, 23:59 AoE Time</li>
                <li>Acceptance Notification: TBD</li>
                <li>Camera-Ready Deadline: TBD</li>
              </ul>
            </div>
      
            <div class="row" style="padding-top: 30px;">
              <h4 style="font-weight: bold; margin-bottom: 15px;">Review Guide</h4>
              <p>Thank you for your interest in the 2nd Workshop on Multi-Modal Reasoning for Agentic Intelligence (MMRAgI) at CVPR 2026. Your expertise and dedication contribute greatly
                to the success of this event.</p>
      
              <p><strong>Review:</strong></p>
              <ul>
                <li><strong>Confidentiality:</strong> All review assignments and the content of the papers you review should
                  be kept confidential. Do not share these materials or discuss them with others unless they are also
                  reviewers for the same paper.</li>
                <li><strong>Conflict of Interest:</strong> If you recognize a conflict of interest with any paper you are
                  assigned to review, please notify the program chairs immediately.</li>
                <li><strong>Length Requirement:</strong> We recommend paper submission within 8 pages (excluding references).
                </li>
                <li><strong>Review Criteria:</strong></li>
                <ul style="list-style-type: circle; padding-left: 20px;">
                  <li>(1) <strong>Relevance:</strong> Does the paper align with the theme of the workshop, i.e.,
                    multi-llm-agent systems?</li>
                  <li>(2) <strong>Originality:</strong> Does the paper present new ideas or results, or does it significantly
                    build upon previous work?</li>
                  <li>(3) <strong>Technical Soundness:</strong> Both position papers and methodology papers are acceptable. Is
                    the opinion or methodology correct and properly explained? Are the claims supported by theoretical
                    analysis or experimental results?</li>
                  <li>(4) <strong>Clarity:</strong> Is the paper well-written and well-structured? Is it easy for readers to
                    understand the problem, the approach, and the results?</li>
                  <li>(5) <strong>Impact:</strong> If the results are applied, do they have the potential to contribute to the
                    advancement of multi-llm-agent systems?</li>
                </ul>
              </ul>
            </div>
          </div>
        </div>
      </section>
      <!-- End Call for Papers Section -->
      
      <!-- ======= Topics Section ======= -->
      <!-- <section id="topics" class="team">
        <div class="container">
          <div class="section-title" data-aos="zoom-out">
            <h2>Topics</h2>
          </div>
          <div class="container">
            <div class="row">
              <p>We welcome submissions addressing the construction, analysis, and application of interactivehuman-centric foundation models, including topics such as:</p>
          </div>
    
          <div class="row">
            <div class="col-md-4">
              <div class="topic-panel interactive-panel">
                <b>Benchmarks and datasets:</b>
                <p>Establishing metrics, datasets, and benchmarks to evaluate the interactivity, multimodal integration, and
                  real-world performance of human-centric foundation models.</p>
              </div>
            </div>
    
            <div class="col-md-4">
              <div class="topic-panel interactive-panel">
                <b>Interactive human-centric foundation models:</b>
                <p>Developing scalable methods that integratehuman-centric priors into perceptual, generative, and
                  embodiment tasks, enabling dynamic userengagement and real-time control.</p>
              </div>
            </div>
    
            <div class="col-md-4">
              <div class="topic-panel interactive-panel">
                <b>Interactive simulation and adaptation systems driven by human-centric foundationmodels:</b>
                <p>Creating systems that leverage human-centric foundation models to support real-timeinteractions, allowing
                  users to explore, intervene, and adapt within complex environments.</p>
              </div>
            </div>
          </div>
    
          <div class="row">
            <div class="col-md-6">
              <div class="topic-panel interactive-panel">
                <b>Scalable multimodal integration in human-centric foundation models:</b>
                <p>Leveraging diversemodalities (vision, language, audio, motion) to build comprehensive models that capture
                  the richcomplexity of human behavior through efficient learning strategies.</p>
              </div>
            </div>
    
            <div class="col-md-6">
              <div class="topic-panel interactive-panel">
                <b>Real-world applications of interactive human-centric foundation models:</b>
                <p>Investigating thedeployment of interactive human-centric foundation models in social robotics, autonomous
                  systems,digital content creation, and beyond to enhance decision-making and user engagement.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End Topics Section -->

    <!-- ======= Speaker Section ======= -->
    <section id="speaker" class="team">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Speakers (tentative)</h2>
        </div>

        <div class="row">
          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/RanjayKrishna.jpeg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://ranjaykrishna.com/index.html" target="_blank">Ranjay Krishna</a></h4>
                <strong>Assistant Professor, University of Washington.</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/Alexander.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://sites.google.com/view/alextoshev" target="_blank">Alexander Toshev</a></h4>
                <strong>Research Scientist and Manager, Apple ML Research.</strong>
              </div>
            </div>
          </div>

          <!-- <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/JamesZou.avif" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.james-zou.com/" target="_blank">James Zou</a></h4>
                <strong>Assistant Professor, Stanford University.</strong>
              </div>
            </div>
          </div> -->
          <!-- <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/cbfinn.jpg" class="img-fluid" style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://ai.stanford.edu/~cbfinn/" target="_blank">Chelsea Finn</a></h4>
                <strong>Assistant Professor, Stanford University.</strong>
              </div>
            </div>
          </div> -->
          <!-- <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/yejinc.jpeg" class="img-fluid" style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://yejinc.github.io/" target="_blank">Yejin Choi</a></h4>
                <strong>Professor, University of Washington. Senior Director, AI2.</strong>
              </div>
            </div>
          </div> -->
          
         
          
          <!-- <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/lijuanwang.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.microsoft.com/en-us/research/people/lijuanw/" target="_blank">Lijuan Wang</a></h4>
                <strong>Principal Research Manager, Microsoft Cloud & AI.</strong>
              </div>
            </div>
          </div> -->
          
          

        


          
          

        </div>
      </div>
    </section>

    <!-- ======= Panelist Section ======= -->
    <!-- <section id="panelist" class="team">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Panelist</h2>
        </div>

        <div class="row">
          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/panelist/tao.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://taoyds.github.io/">Tao Yu</a></h4>
                <strong>Assistant Professor, The University of Hong Kong</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/panelist/roberta.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://rraileanu.github.io/">Roberta Raileanu</a></h4>
                <strong>Research Scientist, Meta GenAI</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/panelist/alexandre.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.alexdrouin.com/">Alexandre Drouin</a></h4>
                <strong>Staff Research Scientist, ServiceNow Research </strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/denny.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://dennyzhou.github.io/">Denny
                    Zhou</a></h4>
                <strong>Principal Scientist/Research Director, Google
                  DeepMind</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="https://www.nextcanada.com/wp-content/uploads/2019/09/graham-neubig.jpg" class="img-fluid"
                  alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.phontron.com/">Graham
                    Neubig</a></h4>
                <strong>Associate Professor, CMU LTI</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/luke.jpeg" class="img-fluid" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.cs.washington.edu/people/faculty/lsz">Luke Zettlemoyer</a></h4>
                <strong>Professor, Allen School of Computer Science & Engineering, University of Washington</strong>
              </div>
            </div>
          </div>



        </div>
    </section> -->

    <!-- ======= Schedule Section ======= -->
    <!-- ======= Workshop Schedule Section ======= -->
    <!-- <section id="schedule" class="schedule">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Workshop Schedule</h2>
        </div>
    
        <div class="row">
          <div class="col-lg-12">
            <div class="schedule-table">
              <table class="table">
                <thead>
                  <tr>
                    <th scope="col">Time</th>
                    <th scope="col">Session</th>
                    <th scope="col">Duration</th>
                    <th scope="col">Details</th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="morning-session"><td>08:50 am - 09:00 am</td><td>Opening Remarks</td><td>10 min</td><td>Welcome and Introduction to the Workshop</td></tr>
                  <tr class="morning-session"><td>09:00 am - 09:30 am</td><td>Invited Talk 1: <b>Words, Worlds and Wheels: The Road Ahead</b></td><td>30 min</td><td>Speaker: Ani Kembhavi</td></tr>
                  <tr class="morning-session"><td>09:30 am - 10:00 am</td><td>Invited Talk 2: <b>Reinforcing World Model Reasoning for VLM Agents</b></td><td>30 min</td><td>Speaker: Manling Li</td></tr>
                  <tr class="morning-session"><td>10:00 am - 10:30 am</td><td>Invited Talk 3: <b>Robot Foundation Models for Open-Ended, Long-Horizon Reasoning</b></td><td>30 min</td><td>Speaker: Lucy Shi</td></tr>
                  <tr class="morning-session"><td>10:45 am - 11:00 am</td><td>Best Paper Talk 1: Achilles Heel of Distributed Multi-Agent Systems </td><td>15 min</td><td>Top rated paper presentation</td></tr>
                  <tr class="morning-session"><td>11:00 am - 11:15 am</td><td>Best Paper Talk 2: Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction</td><td>15 min</td><td>Top rated paper presentation</td></tr>
                  <tr class="morning-session"><td>11:15 am - 12:15 am</td><td>Poster Session  <span style="color: #f76d6d;">(at Exhibit Hall II, #77-84)</span></td><td>60 min</td><td>Networking</td></tr>
                  <!-- <tr class="morning-session"><td>11:00 am - 11:15 am</td><td>Best Paper Talk 2</td><td>15 min</td><td>Top rated paper presentation</td></tr> -->
                  <!-- <tr class="morning-session"><td>12:15 am - 01:05 pm</td><td>Lunch</td><td>50 min</td><td>Food</td></tr>
    
                  <tr class="afternoon-session"><td>01:05 pm - 01:35 pm</td><td>Invited Talk 4: <b>AI Agents: From Language to Multimodal Reasoning </b></td><td>30 min</td><td>Speaker: Juan Carlos Niebles</td></tr>
                  <tr class="afternoon-session"><td>01:35 pm - 02:05 pm</td><td>Invited Talk 5: <b>From LLMs to Generalist Embodied AI Agents: Methods and Lessons</b></td><td>30 min</td><td>Speaker: Alexander Toshev</td></tr>
                  <tr class="afternoon-session"><td>02:05 pm - 02:20 pm</td><td>Best Paper Talk 3: AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning</td><td>15 min</td><td>Top rated paper presentation</td></tr>
                  <tr class="afternoon-session"><td>02:20 pm - 02:35 pm</td><td>Best Paper Talk 4: CDP: Towards Robust Autoregressive Visuomotor Policy Learning via Causal Diffusion</td><td>15 min</td><td>Top rated paper presentation</td></tr>
                  <tr class="afternoon-session"><td>02:35 pm - 03:05 pm</td><td>Invited Talk 6: <b>MolmoAct: Robotics Models need to Reason in Space</b></td><td>30 min</td><td>Speaker: Ranjay Krishna</td></tr>
                  <tr class="afternoon-session"><td>03:05 pm - 03:20 pm</td><td>Break</td><td>15 min</td><td>Refreshments</td></tr>
                  <tr class="afternoon-session"><td>03:20 pm - 03:50 pm</td><td>Invited Talk 7: <b>Show and Tell: Towards AI Coaching Agents</b></td><td>30 min</td><td>Speaker: Kristen Grauman</td></tr>
                  <tr class="afternoon-session"><td>03:50 pm - 04:20 pm</td><td>Invited Talk 8: <b>Advanced frameworks and algorithm for large langauge model reasoning</b></td><td>30 min</td><td>Speaker: Ling Yang</td></tr>
    
                  <tr class="evening-session"><td>04:20 pm - 05:20 pm</td><td>Panel Discussion</td><td>60 min</td><td>Interactive session with panelists</td></tr>
                  <tr class="evening-session"><td>05:20 pm - 05:50 pm</td><td>Breakout Rooms Discussion</td><td>30 min</td><td>Small group discussions</td></tr>
                  <tr class="evening-session"><td>05:50 pm - 06:00 pm</td><td>Closing Remarks</td><td>10 min</td><td>Concluding the workshop</td></tr>
                </tbody>
              </table>
            </div>
          </div>
        </div>
      </div>
    </section> --> -->
    <!-- End Workshop Schedule Section -->

    <!-- ======= CFP Section ======= -->
    <!-- <section id="cfp" class="cfp">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
          <h2>Call for Papers</h2>
        </div>

        <div class="row">
          <div class="col-lg-12">
            <div class="cfp-details">
              <h3>Important Dates:</h3>
              <ul>
                <li><strong>Submission Deadline:</strong> February
                  11th, 2024 (11:59 pm AoE)
                </li>
                <li><strong>Acceptance Notification:</strong>
                  <del>March 3rd, 2024</del> March 10th, 2024
                </li>

                <li><strong>Camera Ready Deadline:</strong> April
                  20th, 2024</li>
                <li><strong>Paper Availability on Website:</strong>
                  April
                  27th, 2024</li>
                <li><strong>Workshop Date:</strong> May 11th,
                  2024</li>
                <li><strong>Location:</strong> Vienna Exhibition &
                  Congress Center</li>
              </ul>

              <h3>Submission Tracks:</h3>

              <p>Consistent with the themes of the workshop, we
                invite
                contributions in the areas <a href="#topics">highlighted above</a>. However, we
                emphasize that the topics list is not exhaustive and
                welcome submissions in related areas. There is no
                need to specify your track on OpenReview. Our
                workshop will not accept work that has been
                previously published in other conferences on machine
                learning. Work that is presented at the main ICLR
                conference should not be submitted to us as well.

              </p>

              <ul>
                <li><strong>Research Paper Track:</strong> We
                  welcome a
                  variety of original research papers, including but
                  not limited to those
                  that propose new techniques, discussion-based
                  papers,
                  literature surveys, and position papers. Research
                  papers can have a <strong>maximum</strong> length
                  of up to 9
                  pages of content, plus unlimited pages
                  for references and appendix.</li>
                <li><strong>Demo Paper Track:</strong> We also
                  welcome
                  technical reports for the demo track, with a
                  <strong>maximum</strong>
                  of 9 pages (same as research papers). In addition
                  to the
                  paper, please provide a link to a video, website,
                  or
                  code
                  repository showcasing your demo.
                </li>
              </ul>

              <h3>Submission Guidelines:</h3>
              <ul>
                <li>üåê Submission Platform:
                  <ul>
                    <li>Submit your papers here: <strong><a
                          href="https://openreview.net/group?id=ICLR.cc/2024/Workshop/LLMAgents">Openreview
                          Submission
                          Site</a></strong></li>
                  </ul>
                </li>
                <li>üìÑ Paper Requirements:
                  <ul>
                    <li>Use the provided <a href="https://github.com/ICLR/Master-Template/raw/master/iclr2024.zip">LaTeX
                        template</a> for your
                      submission.</li>
                    <li>Papers should be anonymized and uploaded as
                      a
                      single PDF.</li>
                    <li>üìö References and Appendix: Reviewers are
                      not
                      obliged to read the appendix.</li>
                  </ul>
                </li>
                <li>üîç Non-Archival Policy:
                  <ul>
                    <li>Submissions will <strong>not be</strong>
                      indexed or have archival proceedings. We
                      welcome ICML 24 or ACL 24 submissions.</li>
                    <li>Accepted papers will be displayed on the
                      workshop website on <strong>27th April
                        2024</strong>. </li>
                  </ul>
                </li>
                <li>üîÑ Dual Submission Policy:
                  <ul>
                    <li>Submissions under review at other venues
                      will
                      be accepted, provided they do not breach any
                      dual-submission or anonymity policies of those
                      venues.</li>
                  </ul>
                </li>
                <li>üëÄ Review Process:
                  <ul>
                    <li>The review process is double-blind.</li>
                  </ul>
                </li>
                <li>üèÜ Best Paper Award:
                  <ul>
                    <li>The award for best paper will be
                      announced at the
                      workshop.</li>
                  </ul>
                </li>
              </ul>

            </div>
          </div>
        </div>

      </div>
    </section> -->
    <!-- End CFP Section -->

    <!-- ======= Accepted Papers Section ======= -->
    <!-- <section id="accepted-papers" class="accepted-papers">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Accepted Papers</h2>
        </div>
        <div class="accordion" id="papersAccordion">
          <div class="accordion-item">
            <h2 class="accordion-header" id="headingOral">
              <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                data-bs-target="#collapseOral" aria-expanded="true" aria-controls="collapseOral">
                Oral Presentations
              </button>
            </h2>
            <div id="collapseOral" class="accordion-collapse collapse" aria-labelledby="headingOral"
              data-bs-parent="#papersAccordion">
              <div class="accordion-body">
                <ul>
                  <li><strong> AutoGen: Enabling Next-Gen LLM
                      Applications via Multi-Agent
                      Conversation</strong>, <br>Qingyun Wu, Gagan
                    Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang
                    Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang,
                    Jiale Liu, Ahmed Hassan Awadallah, Ryen W White,
                    Doug Burger, Chi Wang</li>
                  <li><strong> Data-Copilot: Bridging Billions of
                      Data and Humans with Autonomous
                      Workflow</strong>, <br>Wenqi Zhang, Yongliang
                    Shen, Weiming Lu, Yueting Zhuang</li>
                  <li><strong> AutoAct: Automatic Agent Learning
                      from Scratch via Self-Planning</strong>,
                    <br>Shuofei Qiao, Ningyu Zhang, Runnan Fang,
                    Yujie Luo, Wangchunshu Zhou, Yuchen Eleanor
                    Jiang, chengfei lv, Huajun Chen
                  </li>
                  <li><strong> Large Language Models can
                      Strategically Deceive their Users when Put
                      Under Pressure</strong>, <br>J√©r√©my Scheurer,
                    Mikita Balesni, Marius Hobbhahn</li>
                  <li><strong> Executable Code Actions Elicit Better
                      LLM Agents</strong>, <br>Xingyao Wang, Yangyi
                    Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao
                    Peng, Heng Ji</li>
                  <li><strong> Exploring Collaboration Mechanisms
                      for LLM Agents: A Social Psychology
                      View</strong>, <br>Jintian Zhang, Xin Xu,
                    Ningyu Zhang, Ruibo Liu, Bryan Hooi, Shumin
                    Deng</li>
                </ul>
              </div>
            </div>
          </div>
          <div class="accordion-item">
            <h2 class="accordion-header" id="headingPoster">
              <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                data-bs-target="#collapsePoster" aria-expanded="false" aria-controls="collapsePoster">
                Poster Presentations
              </button>
            </h2>
            <div id="collapsePoster" class="accordion-collapse collapse" aria-labelledby="headingPoster"
              data-bs-parent="#papersAccordion">
              <div class="accordion-body">
                <ul>
                  <li><strong> Towards Unified Alignment Between
                      Agents, Humans, and Environment</strong>,
                    <br>Zonghan Yang, An Liu, Zijun Liu, Kaiming
                    Liu, Fangzhou Xiong, Yile Wang, Zeyuan Yang,
                    Qingyuan Hu, XinRui Chen, Zhenhe Zhang, Fuwen
                    Luo, Zhicheng Guo, Peng Li, Yang Liu
                  </li>
                  <li><strong> Self-Training Language Models in
                      Arithmetic Reasoning</strong>, <br>Marek
                    Kadlƒç√≠k, Michal ≈†tef√°nik, Ondrej Sotolar,
                    Vlastimil Martinek</li>
                  <li><strong> R2E: Turning any Github Repository
                      into a Programming Agent Test
                      Environment</strong>, <br>Naman Jain, Manish
                    Shetty, Tianjun Zhang, King Han, Koushik Sen,
                    Ion Stoica</li>
                  <li><strong> Lumos: Learning Agents with Unified
                      Data, Modular Design, and Open-Source
                      LLMs</strong>, <br>Da Yin, Faeze Brahman,
                    Abhilasha Ravichander, Khyathi Chandu, Kai-Wei
                    Chang, Yejin Choi, Bill Yuchen Lin</li>
                  <li><strong> LEAGUE++: EMPOWERING CONTINUAL ROBOT
                      LEARNING THROUGH GUIDED SKILL ACQUISITION WITH
                      LARGE LANGUAGE MODELS</strong>, <br>Zhaoyi Li,
                    Kelin Yu, Shuo Cheng, Danfei Xu</li>
                  <li><strong> WavCraft: Audio Editing and
                      Generation with Large Language
                      Models</strong>, <br>Jinhua Liang, Huan Zhang,
                    Haohe Liu, Yin Cao, Qiuqiang Kong, Xubo Liu,
                    Wenwu Wang, Mark D Plumbley, Huy Phan, Emmanouil
                    Benetos</li>
                  <li><strong> SAGE: Bridging Semantic and
                      Actionable Parts for Generalizable
                      Manipulation of Articulated Objects</strong>,
                    <br>Haoran Geng, Songlin Wei, Congyue Deng,
                    Bokui Shen, He Wang, Leonidas Guibas
                  </li>
                  <li><strong> Simulating Opinion Dynamics with
                      Networks of LLM-based Agents</strong>,
                    <br>Yun-Shiuan Chuang, Agam Goyal, Nikunj
                    Harlalka, Siddharth Suresh, Robert D. Hawkins,
                    Sijia Yang, Dhavan V. Shah, Junjie Hu, Timothy
                    T. Rogers
                  </li>
                  <li><strong> Agents: An Open-source Framework for
                      Autonomous Language Agents</strong>,
                    <br>Wangchunshu Zhou, Yuchen Eleanor Jiang, Long
                    Li, Jialong Wu, Tiannan Wang, Shuai Wang, Jiamin
                    Chen, Jintian Zhang, Jing Chen, Xiangru Tang,
                    Peng Cui, Ningyu Zhang, Huajun Chen, Mrinmaya
                    Sachan
                  </li>
                  <li><strong> A Human-Inspired Reading Agent with
                      Gist Memory of Very Long Contexts</strong>,
                    <br>Kuang-Huei Lee, Xinyun Chen, Hiroki Furuta,
                    John Canny, Ian Fischer
                  </li>
                  <li><strong> The Agent Ohana: Designing Unified
                      Data and Training Pipeline for Effective Agent
                      Learning</strong>, <br>Jianguo Zhang, Tian
                    Lan, Rithesh R N, Zhiwei Liu, Weiran Yao, Juntao
                    Tan, Yihao Feng, Thai Quoc Hoang, Tulika Manoj
                    Awalgaonkar, Liangwei Yang, Shelby Heinecke,
                    Huan Wang, Juan Carlos Niebles, Silvio Savarese,
                    Caiming Xiong</li>
                  <li><strong> Can Large Language Models be Good
                      Path Planners? A Benchmark and Investigation
                      on Spatial-temporal Reasoning</strong>,
                    <br>Mohamed Aghzal, Erion Plaku, Ziyu Yao
                  </li>
                  <li><strong> FinMem: A Performance-Enhanced LLM
                      Trading Agent with Layered Memory and
                      Character Design</strong>, <br>Haohang Li,
                    Yangyang Yu, Zhi Chen, Yuechen Jiang, Yang Li,
                    Denghui Zhang, Rong Liu, Jordan W. Suchow,
                    Khaldoun Khashanah</li>
                  <li><strong> ArCHer: Training Language Model
                      Agents via Hierarchical Multi-Turn
                      RL</strong>, <br>Yifei Zhou, Andrea Zanette,
                    Jiayi Pan, Aviral Kumar, Sergey Levine</li>
                  <li><strong> Beyond A*: Better LLM planning via
                      Search Dynamics Bootstrapping</strong>,
                    <br>Lucas Lehnert, Sainbayar Sukhbaatar, Paul
                    McVay, Michael Rabbat, Yuandong Tian
                  </li>
                  <li><strong> A-CONECT: Designing AI-based
                      Conversational Chatbot for Early Dementia
                      Intervention</strong>, <br>Junyuan Hong,
                    Wenqing Zheng, Han Meng, Siqi Liang, Anqing
                    Chen, Hiroko H. Dodge, Jiayu Zhou, Zhangyang
                    Wang</li>
                  <li><strong> Agent Smith: A Single Image Can
                      Jailbreak One Million Multimodal LLM Agents
                      Exponentially Fast</strong>, <br>Xiangming Gu,
                    Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu,
                    Ye Wang, Jing Jiang, Min Lin</li>
                  <li><strong> Large Language Model Evaluation Via
                      Multi AI Agents: Preliminary results</strong>,
                    <br>Zeeshan Rasheed, Muhammad Waseem, Kari
                    Syst√§, Pekka Abrahamsson
                  </li>
                  <li><strong> Towards General Computer Control: A
                      Multimodal Agent for Red Dead Redemption II as
                      a Case Study</strong>, <br>Weihao Tan, Ziluo
                    Ding, Wentao Zhang, Boyu Li, Bohan Zhou, Junpeng
                    Yue, Haochong Xia, Jiechuan Jiang, Longtao
                    Zheng, Xinrun Xu, Yifei Bi, Pengjie Gu, Xinrun
                    Wang, B√∂rje F. Karlsson, Bo An, Zongqing Lu</li>
                  <li><strong> GPT-4V(ision) is a Generalist Web
                      Agent, if Grounded</strong>, <br>Boyuan Zheng,
                    Boyu Gou, Jihyung Kil, Huan Sun, Yu Su</li>
                  <li><strong> OpenAgents: An Open Platform for
                      Language Agents in the Wild</strong>,
                    <br>Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng
                    Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua,
                    Junning Zhao, Qian Liu, Che Liu, Zeyu Liu,
                    Yiheng Xu, Hongjin SU, Dongchan Shin, Caiming
                    Xiong, Tao Yu
                  </li>
                  <li><strong> OpenFMNav: Towards Open-Set Zero-Shot
                      Object Navigation via Vision-Language
                      Foundation Models</strong>, <br>Yuxuan Kuang,
                    Hai Lin, Meng Jiang</li>
                  <li><strong> TravelPlanner: A Benchmark for
                      Real-World Planning with Language
                      Agents</strong>, <br>Jian Xie, Kai Zhang,
                    Jiangjie Chen, Tinghui Zhu, Renze Lou, Yuandong
                    Tian, Yanghua Xiao, Yu Su</li>
                  <li><strong> Empowering Autonomous Driving with
                      Large Language Models: A Safety
                      Perspective</strong>, <br>Yixuan Wang, Ruochen
                    Jiao, Simon Sinong Zhan, Chengtian Lang, Chao
                    Huang, Zhaoran Wang, Zhuoran Yang, Qi Zhu</li>
                  <li><strong> REX: Rapid Exploration and
                      eXploitation for AI agents</strong>,
                    <br>Rithesh R N, Shelby Heinecke, Juan Carlos
                    Niebles, Zhiwei Liu, Le Xue, Weiran Yao, Yihao
                    Feng, Zeyuan Chen, Akash Gokul, Devansh Arpit,
                    Ran Xu, Phil L Mui, Huan Wang, Caiming Xiong,
                    Silvio Savarese
                  </li>
                  <li><strong> Towards Natural Language-Driven
                      Industrial Assembly Using Foundation
                      Models</strong>, <br>Omkar Joglekar, Shir
                    Kozlovsky, Tal Lancewicki, Vladimir Tchuiev,
                    Zohar Feldman, Dotan Di Castro</li>
                  <li><strong> Mobile-Agent: Autonomous Multi-Modal
                      Mobile Device Agent with Visual
                      Perception</strong>, <br>Junyang Wang, Haiyang
                    Xu, Jiabo Ye, Ming Yan, Weizhou Shen, Ji Zhang,
                    Fei Huang, Jitao Sang</li>
                  <li><strong> Exposing Limitations of Language
                      Model Agents in Sequential-Task Compositions
                      on the Web</strong>, <br>Hiroki Furuta, Yutaka
                    Matsuo, Aleksandra Faust, Izzeddin Gur</li>
                  <li><strong> LLM Reasoners: New Evaluation,
                      Library, and Analysis of Step-by-Step
                      Reasoning with Large Language Models</strong>,
                    <br>Shibo Hao, Yi Gu, Haotian Luo, Tianyang Liu,
                    Xiyan Shao, Xinyuan Wang, Shuhua Xie, Haodi Ma,
                    Adithya Samavedhi, Qiyue Gao, Zhen Wang, Zhiting
                    Hu
                  </li>
                  <li><strong> R-Judge: Benchmarking Safety Risk
                      Awareness for LLM Agents</strong>, <br>Tongxin
                    Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang,
                    Ruijie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou,
                    Li Fangqi, Zhuosheng Zhang, Rui Wang, Gongshen
                    Liu</li>
                  <li><strong> LLF-Bench: Benchmark for Interactive
                      Learning from Language Feedback</strong>,
                    <br>Ching-An Cheng, Andrey Kolobov, Dipendra
                    Misra, Allen Nie, Adith Swaminathan
                  </li>
                  <li><strong> LLM-Deliberation: Evaluating LLMs
                      with Interactive Multi-Agent Negotiation
                      Game</strong>, <br>Sahar Abdelnabi, Amr Gomaa,
                    Sarath Sivaprasad, Lea Sch√∂nherr, Mario
                    Fritz</li>
                  <li><strong> Is it Possible to Edit Large Language
                      Models Robustly?</strong>, <br>Xinbei Ma,
                    Tianjie Ju, Jiyang Qiu, Zhuosheng Zhang, hai
                    zhao, lifeng Liu, Yulong Wang</li>
                  <li><strong> Agent Instructs Large Language Models
                      to be General Zero-Shot Reasoners</strong>,
                    <br>Nicholas Crispino, Kyle Montgomery, Fankun
                    Zeng, Dawn Song, Chenguang Wang
                  </li>
                  <li><strong> WorkArena: How Capable are Web Agents
                      at Solving Common Knowledge Work
                      Tasks?</strong>, <br>Alexandre Drouin, Maxime
                    Gasse, Massimo Caccia, Issam H. Laradji, Manuel
                    Del Verme, Tom Marty, David Vazquez, Nicolas
                    Chapados, Alexandre Lacoste</li>
                  <li><strong> Corex: Pushing the Boundaries of
                      Complex Reasoning through Multi-Model
                      Collaboration</strong>, <br>Qiushi Sun,
                    Zhangyue Yin, Xiang Li, Zhiyong Wu, Xipeng Qiu,
                    Lingpeng Kong</li>
                  <li><strong> ProtAgents: Protein discovery via
                      large language model multi-agent
                      collaborations combining physics and machine
                      learning</strong>, <br>Alireza Ghafarollahi,
                    Markus Buehler</li>
                  <li><strong> Hierarchical Auto-Organizing System
                      for Open-Ended Multi-Agent
                      Navigation</strong>, <br>Zhonghan Zhao, Kewei
                    Chen, Dongxu Guo, Wenhao Chai, Tian Ye, Yanting
                    Zhang, Gaoang Wang</li>
                  <li><strong> EHRAgent: Code Empowers Large
                      Language Models for Few-shot Complex Tabular
                      Reasoning on Electronic Health
                      Records</strong>, <br>Wenqi Shi, Ran Xu,
                    Yuchen Zhuang, Yue Yu, Jieyu Zhang, Hang Wu,
                    Yuanda Zhu, Joyce C. Ho, Carl Yang, May Dongmei
                    Wang</li>
                  <li><strong> Uncertainty of Thoughts:
                      Uncertainty-Aware Planning Enhances
                      Information Seeking in Large Language
                      Models</strong>, <br>Zhiyuan Hu, Chumin Liu,
                    Xidong Feng, Yilun Zhao, See-Kiong Ng, Anh Tuan
                    Luu, Junxian He, Pang Wei Koh, Bryan Hooi</li>
                  <li><strong> TaskBench: Benchmarking Large
                      Language Models for Task Automation</strong>,
                    <br>Yongliang Shen, Kaitao Song, Xu Tan, Wenqi
                    Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng
                    Li, Yueting Zhuang
                  </li>
                  <li><strong> SELF-IMAGINE: Effective Unimodal
                      Reasoning with Multimodal Models using
                      Self-Imagination</strong>, <br>Syeda Nahida
                    Akter, Aman Madaan, Sangwu Lee, Yiming Yang,
                    Eric Nyberg</li>
                  <li><strong> BioDiscoveryAgent: An AI Agent for
                      Designing Genetic Perturbation
                      Experiments</strong>, <br>Yusuf H Roohani,
                    Jian Vora, Qian Huang, Percy Liang, Jure
                    Leskovec</li>
                  <li><strong> MAGIC: INVESTIGATION OF LARGE
                      LANGUAGE MODEL POWERED MULTI-AGENT IN
                      COGNITION, ADAPTABILITY, RATIONALITY AND
                      COLLABORATION</strong>, <br>Lin Xu, Zhiyuan
                    Hu, Daquan Zhou, Hongyu Ren, Zhen Dong, Kurt
                    Keutzer, See-Kiong Ng, Jiashi Feng</li>
                  <li><strong> Do LLM Agents Have Regret? A Case
                      Study in Online Learning and Games</strong>,
                    <br>Chanwoo Park, Xiangyu Liu, Asuman E.
                    Ozdaglar, Kaiqing Zhang
                  </li>
                  <li><strong> Prioritizing Safeguarding Over
                      Autonomy: Risks of LLM Agents for
                      Science</strong>, <br>Xiangru Tang, Qiao Jin,
                    Kunlun Zhu, Tongxin Yuan, Yichi Zhang,
                    Wangchunshu Zhou, Meng Qu, Yilun Zhao, Jian
                    Tang, Zhuosheng Zhang, Arman Cohan, Zhiyong Lu,
                    Mark Gerstein</li>
                  <li><strong> Expressing and Exploiting Parallelism
                      in Language Model Decoding</strong>, <br>Tian
                    Jin, Ellie Y Cheng, Michael Carbin</li>
                  <li><strong> Towards Self-Improving Language
                      Models for Code Generation</strong>,
                    <br>Micha√´l Defferrard, Corrado Rainone, David
                    W. Zhang, Blazej Manczak, Natasha Butt, Taco
                    Cohen
                  </li>
                  <li><strong> MathChat: Converse to Tackle
                      Challenging Math Problems with LLM
                      Agents</strong>, <br>Yiran Wu, Feiran Jia,
                    Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang,
                    Yin Tat Lee, Richard Peng, Qingyun Wu, Chi
                    Wang</li>
                  <li><strong> L3GO: Language Agents with
                      Chain-of-3D-Thoughts for Generating
                      Unconventional Objects</strong>, <br>Yutaro
                    Yamada, Khyathi Chandu, Bill Yuchen Lin, Jack
                    Hessel, Ilker Yildirim, Yejin Choi</li>
                  <li><strong> An Embodied Generalist Agent in 3D
                      World</strong>, <br>Jiangyong Huang, Silong
                    Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li,
                    Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia,
                    Siyuan Huang</li>
                  <li><strong> Agent-Pro: Learning to Evolve via
                      Policy-Level Reflection and
                      Optimization</strong>, <br>Wenqi Zhang, Ke
                    Tang, Hai Wu, Mengna Wang, Yongliang Shen,
                    Guiyang Hou, Zeqi Tan, Peng Li, Yueting Zhuang,
                    Weiming Lu</li>
                  <li><strong> Recursive Speculative Decoding:
                      Accelerating LLM Inference via Sampling
                      Without Replacement</strong>, <br>Wonseok
                    Jeon, Mukul Gagrani, Raghavv Goel, Junyoung
                    Park, Mingu Lee, Christopher Lott</li>
                  <li><strong> VisualWebArena: Evaluating Multimodal
                      Agents on Realistic Visual Web Tasks</strong>,
                    <br>Jing Yu Koh, Robert Lo, Lawrence Jang,
                    Vikram Duvvur, Ming Chong Lim, Po-Yu Huang,
                    Graham Neubig, Shuyan Zhou, Ruslan
                    Salakhutdinov, Daniel Fried
                  </li>
                  <li><strong> HELPER-X: A Unified Instructable
                      Embodied Agent to Tackle Four Interactive
                      Vision-Language Domains with Memory-Augmented
                      Language Models</strong>, <br>Gabriel Herbert
                    Sarch, Sahil Somani, Raghav Kapoor, Michael J.
                    Tarr, Katerina Fragkiadaki</li>
                  <li><strong> Controlling Large Language
                      Model-based Agents for Large-Scale
                      Decision-Making: An Actor-Critic
                      Approach</strong>, <br>Bin Zhang, Hangyu Mao,
                    Jingqing Ruan, Ying Wen, Yang Li, Shao Zhang,
                    Zhiwei Xu, Dapeng Li, Ziyue Li, Rui Zhao, Lijuan
                    Li, Guoliang Fan</li>
                  <li><strong> Plan-Seq-Learn: Language Model Guided
                      RL for Solving Long Horizon Robotics
                      Tasks</strong>, <br>Murtaza Dalal, Tarun
                    Chiruvolu, Devendra Singh Chaplot, Ruslan
                    Salakhutdinov</li>
                  <li><strong> Adapting Uni-Modal Language Models
                      for Dense Multi-Modal Co-Reference Resolution
                      using Parameter Augmentation</strong>,
                    <br>Samuel Osebe, Prashan Wanigasekara, Thanh
                    Tran, Thomas Gueudre
                  </li>
                  <li><strong> Preference-Conditioned
                      Language-Guided Abstraction</strong>, <br>Andi
                    Peng, Andreea Bobu, Belinda Z. Li, Theodore
                    Sumers, Ilia Sucholutsky, Nishanth Kumar, Thomas
                    L. Griffiths, Julie Shah</li>
                  <li><strong> S-Agent: self-organizing agents in
                      open-ended environment</strong>, <br>Jiaqi
                    Chen, Yuxian Jiang, Jiachen Lu, Li Zhang</li>
                  <li><strong> Efficient Human-AI Coordination via
                      Preparatory Language-based
                      Convention</strong>, <br>Cong Guan, Lichao
                    Zhang, Chunpeng Fan, Yi-Chen Li, Feng Chen, Lihe
                    Li, Yunjia Tian, Lei Yuan, Yang Yu</li>
                  <li><strong> SeeClick: Harnessing GUI Grounding
                      for Advanced Visual GUI Agents</strong>,
                    <br>Kanzhi Cheng, Qiushi Sun, Yougang Chu,
                    Fangzhi Xu, Li YanTao, Jianbing Zhang, Zhiyong
                    Wu
                  </li>
                  <li><strong> The ART of LLM Refinement: Ask,
                      Refine, Trust</strong>, <br>Kumar
                    Shridhar</li>
                  <li><strong> SceneCraft: An LLM Agent for
                      Synthesizing 3D Scene as Blender
                      Code</strong>, <br>Ziniu Hu</li>
                  <li><strong> LangProp: A code optimization
                      framework using Large Language Models applied
                      to driving</strong>, <br>Shu Ishida, Gianluca
                    Corrado, George Fedoseev, Hudson Yeo, Lloyd
                    Russell, Jamie Shotton, Joao F. Henriques,
                    Anthony Hu</li>
                  <li><strong> FL-TAC: Enhanced Fine-Tuning in
                      Federated Learning via Low-Rank, Task-Specific
                      Adapter Clustering</strong>, <br>Siqi Ping,
                    Yuzhu Mao, Yang Liu, Xiao-Ping Zhang, Wenbo
                    Ding</li>
                  <li><strong> EcoAssistant: Using LLM Assistants
                      More Affordably and Accurately</strong>,
                    <br>Jieyu Zhang, Ranjay Krishna, Ahmed Hassan
                    Awadallah, Chi Wang
                  </li>
                  <li><strong> IntentGPT: Few-shot Intent Discovery
                      with Large Language Models</strong>, <br>Juan
                    A. Rodriguez, Nicholas Botzer, David Vazquez,
                    Christopher Pal, Marco Pedersoli, Issam H.
                    Laradji</li>
                  <li><strong> Language-guided Skill Learning with
                      Temporal Variational Inference</strong>,
                    <br>Haotian Fu, Pratyusha Sharma, Elias
                    Stengel-Eskin, George Konidaris, Nicolas Le
                    Roux, Marc-Alexandre C√¥t√©, Xingdi Yuan
                  </li>
                  <li><strong> Decision-Oriented Dialogue for
                      Human-AI Collaboration</strong>, <br>Jessy
                    Lin, Nicholas Tomlin, Jacob Andreas, Jason
                    Eisner</li>
                  <li><strong> Making Retrieval-Augmented Language
                      Models Robust to Irrelevant Context</strong>,
                    <br>Ori Yoran, Tomer Wolfson, Ori Ram, Jonathan
                    Berant
                  </li>
                  <li><strong> MedAgents: Large Language Models as
                      Collaborators for Zero-shot Medical
                      Reasoning</strong>, <br>Xiangru Tang, Anni
                    Zou, Zhuosheng Zhang, Ziming Li, Yilun Zhao,
                    Xingyao Zhang, Arman Cohan, Mark Gerstein</li>
                  <li><strong> Collaborative LLM-Agents for Editable
                      Driving Scene Simulation</strong>, <br>Yuxi
                    Wei, Zi Wang, Yifan Lu, Chenxin Xu, Changxing
                    Liu, Hao Zhao, Siheng Chen, Yanfeng Wang</li>
                  <li><strong> WebLINX: Real-World Website
                      Navigation with Multi-Turn Dialogue</strong>,
                    <br>Xing Han Lu, Zdenƒõk Kasner, Siva Reddy
                  </li>
                  <li><strong> The Wisdom of Partisan Crowds:
                      Comparing Collective Intelligence in Humans
                      and LLM-based Agents</strong>, <br>Yun-Shiuan
                    Chuang, Nikunj Harlalka, Siddharth Suresh, Agam
                    Goyal, Robert D. Hawkins, Sijia Yang, Dhavan V.
                    Shah, Junjie Hu, Timothy T. Rogers</li>
                  <li><strong> BOLAA: BENCHMARKING AND ORCHESTRATING
                      LLM AUTONOMOUS AGENTS</strong>, <br>Zhiwei
                    Liu, Weiran Yao, Jianguo Zhang, Le Xue, Shelby
                    Heinecke, Rithesh R N, Yihao Feng, Zeyuan Chen,
                    Juan Carlos Niebles, Devansh Arpit, Ran Xu, Phil
                    L Mui, Huan Wang, Caiming Xiong, Silvio
                    Savarese</li>
                  <li><strong> Boosting Task Planning and Tool Usage
                      of Large Language Model-based Agents in
                      Real-world Systems</strong>, <br>Yilun Kong,
                    Jingqing Ruan, YiHong Chen, Bin Zhang, Tianpeng
                    Bao, shi shiwei, du guo qing, xiaoru hu, Hangyu
                    Mao, Ziyue Li, Xingyu Zeng, Rui Zhao, Xueqian
                    Wang</li>
                  <li><strong> Self-Alignment of Large Language
                      Models via Multi-Agent Social
                      Simulation</strong>, <br>Xianghe Pang, Shuo
                    Tang, Rui Ye, Yuxin Xiong, Bolun Zhang, Yanfeng
                    Wang, Siheng Chen</li>
                  <li><strong> If LLM Is the Wizard, Then Code Is
                      the Wand: A Survey on How Code Empowers Large
                      Language Models to Serve as Intelligent
                      Agents</strong>, <br>Ke Yang, Jiateng Liu,
                    John Wu, Chaoqi Yang, Yi Fung, Sha Li, Zixuan
                    Huang, Xu Cao, Xingyao Wang, Heng Ji, ChengXiang
                    Zhai</li>
                  <li><strong> ReST meets ReAct: Self-Improvement
                      for Multi-Step Reasoning LLM Agent</strong>,
                    <br>Renat Aksitov, Sobhan Miryoosefi, Zonglin
                    Li, Daliang Li, Sheila Babayan, Kavya Kopparapu,
                    Zachary Fisher, Ruiqi Guo, Sushant Prakash,
                    Pranesh Srinivasan, Manzil Zaheer, Felix Yu,
                    Sanjiv Kumar
                  </li>
                  <li><strong> Are Machines Better at Slow Thinking?
                      Unveiling Human-Machine Inference Gaps in
                      Entailment Verification</strong>, <br>Soumya
                    Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya Wang,
                    Xiang Ren</li>
                  <li><strong> Limitations of Agents Simulated by
                      Predictive Models</strong>, <br>Raymond
                    Douglas, Jacek Karwowski, Chan Bae, Andis
                    Draguns, Victoria Krakovna</li>
                  <li><strong> OS-Copilot: Towards Generalist
                      Computer Agents with
                      Self-Improvement</strong>, <br>Zhiyong Wu,
                    Chengcheng Han, Zichen Ding, Zhenmin Weng,
                    Zhoumianze Liu, Shunyu Yao, Tao Yu, Lingpeng
                    Kong</li>
                  <li><strong> EASYTOOL: Enhancing LLM-based Agents
                      with Concise Tool Instruction</strong>,
                    <br>Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu
                    Tan, Yongliang Shen, Kan Ren, Dongsheng Li,
                    Deqing Yang
                  </li>
                  <li><strong> FLASK: Fine-grained Language Model
                      Evaluation based on Alignment Skill
                      Sets</strong>, <br>Seonghyeon Ye, Doyoung Kim,
                    Sungdong Kim, Hyeonbin Hwang, Seungone Kim,
                    Yongrae Jo, James Thorne, Juho Kim, Minjoon
                    Seo</li>
                  <li><strong> Language Agent Tree Search Unifies
                      Reasoning Acting and Planning in Language
                      Models</strong>, <br>Andy Zhou, Kai Yan,
                    Michal Shlapentokh-Rothman, Haohan Wang,
                    Yu-Xiong Wang</li>
                  <li><strong> On the Road with GPT-4V(ision):
                      Explorations of Utilizing Visual-Language
                      Model as Autonomous Driving Agent</strong>,
                    <br>Licheng Wen, Xuemeng Yang, Daocheng Fu,
                    Xiaofeng Wang, Pinlong Cai, Xin Li, Tao MA,
                    Yingxuan Li, Linran XU, Dengke Shang, Zheng Zhu,
                    Shaoyan Sun, Yeqi BAI, Xinyu Cai, Min Dou,
                    Shuanglu Hu, Botian Shi, Yu Qiao
                  </li>
                  <li><strong> Bring Your Own KG: Self-Supervised
                      Program Synthesis for Zero-Shot KGQA</strong>,
                    <br>Dhruv Agarwal, Rajarshi Das, Sopan Khosla,
                    Rashmi Gangadharaiah
                  </li>
                  <li><strong> Open-TI: Open Traffic Intelligence
                      with Augmented Language Model</strong>,
                    <br>Longchao Da, Kuan-Ru Liou, Tiejin Chen,
                    Xuesong Zhou, Xiangyong Luo, Yezhou Yang, Hua
                    Wei
                  </li>
                  <li><strong> AgentBoard: An Analytical Evaluation
                      Board of Multi-turn LLM Agents</strong>,
                    <br>Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng
                    Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan,
                    Lingpeng Kong, Junxian He
                  </li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End Accepted Papers Section -->

    <!-- ======= Organization Section ======= -->
    <section id="org" class="team">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
          <h2>Organization</h2>
          <p>Workshop Organizers</p>
        </div>

        <!-- <div class="section-title" data-aos="zoom-out">
          <h3>Organizing Commitee</h3>
        </div> -->

        <div class="row">
          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/yijiang.png" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://williamium3000.github.io/" target="_blank">William Yijiang Li</a></h4>
                <strong>Ph.D, UC San Diego.</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/zhenfei.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://yinzhenfei.github.io/" target="_blank">Zhenfei Yin</a></h4>
                <strong>Rising Star Researcher, Ph.D, University of Sydney, Postdoctoral Researcher, Oxford.</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/lucyshi.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://lucys0.github.io/" target="_blank">Lucy Shi</a></h4>
                <strong>Ph.D, Stanford University.</strong>
              </div>
            </div>
          </div>


      
          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/Annie.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://anniesch.github.io/" target="_blank">Annie S. Chen</a></h4>
                <strong>Ph.D, Stanford University.</strong>
              </div>
            </div>
          </div>


          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/zixianma.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://zixianma.github.io/" target="_blank">Zixian Ma</a></h4>
                <strong>Ph.D, the University of Washington.</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/MahtabBigverdi.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://mahtabbigverdi.github.io/" target="_blank">Mahtab Bigverdi</a></h4>
                <strong>Ph.D, the University of Washington.</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/AmitaKamath.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://amitakamath.github.io/" target="_blank">Amita Kamath</a></h4>
                <strong>Ph.D, the University of Washington and the University of California, Los Angeles.</strong>
              </div>
            </div>
          </div>
          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/AndaRalucaEpure.webp" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.bdi.ox.ac.uk/Team/anda-raluca-epure" target="_blank">Anda Raluca Epure</a></h4>
                <strong>Ph.D, University of Oxford.</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/Alexander.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://sites.google.com/view/alextoshev" target="_blank">Alexander Toshev</a></h4>
                <strong>Research Scientist and Manager, Apple ML Research.</strong>
              </div>
            </div>
          </div>

          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/speaker/RanjayKrishna.jpeg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://ranjaykrishna.com/index.html" target="_blank">Ranjay Krishna</a></h4>
                <strong>Assistant Professor, University of Washington.</strong>
              </div>
            </div>
          </div>




          <div class="col-lg-2 col-md-6 d-flex align-items-stretch">
            <div class="member" data-aos="fade-up">
              <div class="member-img">
                <img src="assets/img/organizer/PhilipTorr.jpg" class="img-fluid"
                  style="width: 100%; height: 200px; object-fit: cover;" alt>
              </div>
              <div class="member-info">
                <h4><a href="https://www.robots.ox.ac.uk/~phst/" target="_blank">Philip Torr</a></h4>
                <strong>Professor, University of Oxford.</strong>
              </div>
            </div>
          </div>
        </div>
      </div>

    </section><!-- End Organization Section -->

    <!-- ======= Reviewers Section ======= -->
    <!-- <section id="reviewers" class="reviewers">
      <div class="container">
        <div class="section-title" data-aos="zoom-out">
          <h2>Program Committee</h2>
        </div>
        <div class="accordion" id="reviewersAccordion">
          <div class="accordion-item">
            <h2 class="accordion-header" id="headingReviewer">
              <button class="accordion-button collapsed" type="button" data-bs-toggle="collapse"
                data-bs-target="#collapseReviewer" aria-expanded="true" aria-controls="collapseReviewer">
                Reviewers
              </button>
            </h2>
            <div id="collapseReviewer" class="accordion-collapse collapse" aria-labelledby="headingReviewer"
              data-bs-parent="#papersAccordion">
              <div class="accordion-body">

                <div class="row">
                  <div class="col-lg-12">
                    <ul class="list-unstyled">
                      <li>Yuelyu Ji, <it>University of
                          Pittsburgh</it>
                      </li>
                      <li>Hangyu Mao, <it>Sensetime
                          Research</it>
                      </li>
                      <li>Boyuan Zheng, <it>Ohio State University,
                          Columbus</it>
                      </li>
                      <li>Siyu Yuan, <it>Fudan University</it>
                      </li>
                      <li>Xin Cong, <it>Tsinghua University,
                          Tsinghua
                          University</it>
                      </li>
                      <li>Markus Buehler, <it>Massachusetts
                          Institute of
                          Technology</it>
                      </li>
                      <li>Lin Xu, <it>National University of
                          Singapore</it>
                      </li>
                      <li>Chenfei Yuan, <it>Department of Computer
                          Science
                          and Technology, Tsinghua
                          University</it>
                      </li>
                      <li>Haochen Vector Zhao, <it>Peking
                          University</it>
                      </li>
                      <li>Feiran Jia, <it>Pennsylvania State
                          University</it>
                      </li>
                      <li>Yao Yao, <it>Shanghai Jiaotong
                          University</it>
                      </li>
                      <li>Zhang Ruichen, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Mathieu Ravaut, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Zirui Zhao, <it>national university of
                          singaore,
                          National University of Singapore</it>
                      </li>
                      <li>Jialong Wu, <it>Southeast
                          University</it>
                      </li>
                      <li>Rithesh R N, <it>SalesForce.com</it>
                      </li>
                      <li>Juntao Tan, <it>Rutgers
                          University</it>
                      </li>
                      <li>Ting Chen, <it>University of Electronic
                          Science
                          and Technology of China</it>
                      </li>
                      <li>Yun-Shiuan Chuang, <it>University of
                          Wisconsin -
                          Madison</it>
                      </li>
                      <li>Jiageng Mao, <it>University of Southern
                          California</it>
                      </li>
                      <li>Yongliang Shen, <it>Microsoft</it>
                      </li>
                      <li>Zhiruo Wang, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Jiuzhou Han, <it>Monash
                          University</it>
                      </li>
                      <li>Kaixin Ma, <it>Tencent AI Lab</it>
                      </li>
                      <li>Hao Peng, <it>Department of Computer
                          Science,
                          University of Illinois
                          Urbana-Champaign</it>
                      </li>
                      <li>Jian Guan, <it>Tsinghua University,
                          Tsinghua
                          University</it>
                      </li>
                      <li>Shaoguang Mao, <it>Microsoft</it>
                      </li>
                      <li>Olivia Watkins, <it>University of
                          California
                          Berkeley</it>
                      </li>
                      <li>Jiateng Liu, <it>Department of Computer
                          Science</it>
                      </li>
                      <li>Qian Huang, <it>Google</it>
                      </li>
                      <li>Haozhe Zhao, <it>Peking
                          University</it>
                      </li>
                      <li>Yecheng Jason Ma, <it>University of
                          Pennsylvania</it>
                      </li>
                      <li>Zhenran Xu, <it>Harbin Institute of
                          Technology,
                          Shenzhen</it>
                      </li>
                      <li>Zhongshen Zeng, <it>Department of Computer
                          Science
                          and Engineering, The Chinese University of
                          Hong
                          Kong</it>
                      </li>
                      <li>Kuang-Huei Lee, <it>Google</it>
                      </li>
                      <li>Chunyuan Deng, <it>Georgia Institute of
                          Technology</it>
                      </li>
                      <li>Meghana Moorthy Bhat, <it>Salesforce
                          Research</it>
                      </li>
                      <li>Tianjun Zhang, <it>University of
                          California
                          Berkeley</it>
                      </li>
                      <li>Jiangyong Huang, <it>Peking
                          University</it>
                      </li>
                      <li>Wenshan Wu, <it>Microsoft</it>
                      </li>
                      <li>Kimin Lee, <it>Korea Advanced Institute of
                          Science
                          & Technology</it>
                      </li>
                      <li>Daquan Zhou, <it>Bytedance</it>
                      </li>
                      <li>Haoqi Yuan, <it>Peking
                          University</it>
                      </li>
                      <li>Osbert Bastani, <it>University of
                          Pennsylvania</it>
                      </li>
                      <li>Shuyan Zhou, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Agam Goyal, <it>University of Wisconsin -
                          Madison</it>
                      </li>
                      <li>Gang Qiao, <it>Siemens
                          Healthineers</it>
                      </li>
                      <li>Xun Wang, <it>Microsoft</it>
                      </li>
                      <li>Sahitya Potluri, <it>Google</it>
                      </li>
                      <li>Xingyao Wang, <it>Department of Computer
                          Science,
                          University of Illinois
                          Urbana-Champaign</it>
                      </li>
                      <li>Wenyue Hua, <it>Rutgers University, New
                          Brunswick</it>
                      </li>
                      <li>Younggyo Seo, <it>Dyson</it>
                      </li>
                      <li>Zhangcheng Qiang, <it>Australian National
                          University</it>
                      </li>
                      <li>Boyu Gou, <it>Ohio State University,
                          Columbus</it>
                      </li>
                      <li>Jian Xie, <it>Fudan University</it>
                      </li>
                      <li>Ziniu Hu, <it>California Institute of
                          Technology</it>
                      </li>
                      <li>Yichi Zhang, <it>Peking
                          University</it>
                      </li>
                      <li>Fangkai Jiao, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Yangyi Chen, <it>School of Computer
                          Science,
                          University of Illinois at
                          Urbana-Champaign</it>
                      </li>
                      <li>Ravi Pandya, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Zelong Li, <it>Rutgers University, New
                          Brunswick</it>
                      </li>
                      <li>Jiayuan Mao, <it>Massachusetts Institute
                          of
                          Technology</it>
                      </li>
                      <li>Bohan Lyu, <it>Tsinghua
                          University</it>
                      </li>
                      <li>Senbao Shi, <it>Harbin Institute of
                          Technology</it>
                      </li>
                      <li>Kaitao Song, <it>Microsoft</it>
                      </li>
                      <li>Nikunj Harlalka, <it>University of
                          Wisconsin -
                          Madison</it>
                      </li>
                      <li>Zhihan Liu, <it>Northwestern
                          University</it>
                      </li>
                      <li>Haochun Wang, <it>Harbin Institute of
                          Technology</it>
                      </li>
                      <li>Chi Zhang, <it>Tencent </it>
                      </li>
                      <li>Chang Gao, <it>The Chinese University of
                          Hong
                          Kong</it>
                      </li>
                      <li>Kun Shao, <it>Huawei Noah's Ark
                          Lab</it>
                      </li>
                      <li>Lanqing Li, <it>Zhejiang Lab</it>
                      </li>
                      <li>Ziyuan Qin, <it>Case Western Reserve
                          University</it>
                      </li>
                      <li>Chengjie Zheng, <it>University of
                          Massachusetts
                          Boston</it>
                      </li>
                      <li>Bharat Prakash, <it>University of
                          Maryland,
                          Baltimore County</it>
                      </li>
                      <li>Yanjun Shao, <it>Fudan
                          University</it>
                      </li>
                      <li>Amrita Saha, <it>SalesForce.com</it>
                      </li>
                      <li>Ke Yang, <it>Department of Computer
                          Science</it>
                      </li>
                      <li>Zhao Xu, <it>Hong Kong University of
                          Science and
                          Technology</it>
                      </li>
                      <li>Ruochen Zhao, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Chaoqi Yang, <it>University of Illinois
                          Urbana
                          Champaign</it>
                      </li>
                      <li>Hao Wang, <it>Google</it>
                      </li>
                      <li>Yangyang Yu, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>Shuofei Qiao, <it>Zhejiang
                          University</it>
                      </li>
                      <li>Hailin Chen, <it>National Technological
                          University</it>
                      </li>
                      <li>Yuan Yao, <it>Nanjing University</it>
                      </li>
                      <li>Lei Liu, <it>The Chinese University of
                          Hong Kong,
                          Shenzhen</it>
                      </li>
                      <li>Yuechen Jiang, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>Pengguang Chen, <it>SmartMore</it>
                      </li>
                      <li>Chuan Xiao, <it>Osaka University</it>
                      </li>
                      <li>Sha Li, <it>University of Illinois, Urbana
                          Champaign</it>
                      </li>
                      <li>Wenqi Zhang, <it>Zhejiang
                          University</it>
                      </li>
                      <li>Yilun Zhao, <it>Yale University</it>
                      </li>
                      <li>Kaikai An, <it>Peking University</it>
                      </li>
                      <li>Yunhao Yang, <it>University of Texas at
                          Austin</it>
                      </li>
                      <li>Haohang Li, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>Jianghao Zhang, <it>University of Michigan
                          - Ann
                          Arbor</it>
                      </li>
                      <li>Shruti Singh, <it>IIT
                          Gandhinagar</it>
                      </li>
                      <li>Zhi Chen, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>He Zhu, <it>Rutgers University</it>
                      </li>
                      <li>Allen Nie, <it>Stanford
                          University</it>
                      </li>
                      <li>Shuzheng Si, <it>Peking
                          University</it>
                      </li>
                      <li>Muhammad Waseem, <it>University of
                          Jyv√§skyl√§</it>
                      </li>
                      <li>Jing Yu Koh, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Kunlun Zhu, <it>Universit√© de
                          Montr√©al</it>
                      </li>
                      <li>Chengwei Qin, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Zengqing Wu, <it>Kyoto
                          University</it>
                      </li>
                      <li>Vernon Bumgardner, <it>University of
                          Kentucky</it>
                      </li>
                      <li>Chenyang Zhao, <it>Zhejiang Lab</it>
                      </li>
                      <li>Rong Liu, <it>Stevens Institute of
                          Technology</it>
                      </li>
                      <li>Sihao Hu, <it>Georgia Institute of
                          Technology</it>
                      </li>
                      <li>Srijan Bansal, <it>Carnegie Mellon
                          University</it>
                      </li>
                      <li>Da Yin, <it>University of California, Los
                          Angeles</it>
                      </li>
                      <li>Hung Le, <it>Salesforce Research</it>
                      </li>
                      <li>Enxhell Luzhnica, <it>Google</it>
                      </li>
                      <li>Michelle D Zhao, <it>CMU, Carnegie Mellon
                          University</it>
                      </li>
                      <li>Yunfan Jiang, <it>Stanford
                          University</it>
                      </li>
                      <li>Hongyang Du, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Jason Phang, <it>New York
                          University</it>
                      </li>
                      <li>Xingxuan Li, <it>Nanyang Technological
                          University</it>
                      </li>
                      <li>Mingqi Gao, <it>Peking
                          University</it>
                      </li>
                      <li>Xiao Han, <it>Peking University</it>
                      </li>
                      <li>Haojie Pan, <it>Department of Computer
                          Science and
                          Engineering, Hong Kong University of
                          Science and
                          Technology</it>
                      </li>
                      <li>Pekka Abrahamsson, <it>Tampere
                          University</it>
                      </li>
                      <li>Haibin Huang, <it>Kuaishou
                          Technology</it>
                      </li>
                      <li>Yiming Zhang, <it>Tokyo Institute of
                          Technology,
                          Tokyo Institute of Technology</it>
                      </li>
                      <li>Baotian Hu, <it>Harbin Institute of
                          Technology,
                          Shenzhen</it>
                      </li>
                      <li>Yang Yuan, <it>Tsinghua University,
                          Tsinghua
                          University</it>
                      </li>
                      <li>Yixin Zhang, <it>Kyoto University, Kyoto
                          University</it>
                      </li>
                      <li>Riccardo Cantini, <it>University of
                          Calabria</it>
                      </li>
                      <li>Tiankai Hang, <it>Southeast
                          University</it>
                      </li>
                      <li>Gongshen Liu, <it>Shanghai Jiao Tong
                          University</it>
                      </li>
                      <li>Yuzhou Du, <it>Northwestern
                          University</it>
                      </li>
                      <li>Xiaocheng Lu, <it>Hong Kong Polytechnic
                          University</it>
                      </li>
                      <li>Sarang Gupta, <it>Asana</it>
                      </li>
                      <li>Inderjeet Jayakumar Nair, <it>University
                          of
                          Michigan - Ann Arbor</it>
                      </li>
                      <li>Gabrielle Kaili-May Liu, <it>Department of
                          Computer Science, Yale
                          University</it>
                      </li>
                      <li>Shuyuan Zheng, <it>Osaka
                          University</it>
                      </li>
                      <li>Run Peng, <it>University of Michigan - Ann
                          Arbor</it>
                      </li>
                      <li>Mira Moukheiber, <it>Massachusetts
                          Institute of
                          Technology</it>
                      </li>
                      <li>John Wu, <it>University of Illinois at
                          Urbana-Champaign</it>
                      </li>
                      <li>Bin Liu, <it>Zhejiang Lab</it>
                      </li>
                    </ul>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section> -->
    <!-- End Reviewers Section -->




    <!-- ======= Contact Section ======= -->
    <!-- <section id="award" class="award">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
          <h2>Award</h2>
        </div>
        <div>
          <h5>The Best Paper Award:
            <br><br>
            AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation

            <br><br>
            Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun
            Zhang, Jiale Liu, Ahmed Hassan Awadallah, Ryen W White, Doug Burger, Chi Wang

          </h5>

        </div>

      </div>

    </section> -->
    <!-- End Contact Section -->




    <!-- ======= Contact Section ======= -->
    <section id="contact" class="contact">
      <div class="container">
    
        <div class="section-title" data-aos="zoom-out">
          <h2>Contact us</h2>
        </div>
        <div>
          <h5>Email us at <a href="mailto:yijiangli@ucsd.edu">yijiangli@ucsd.edu</a>
            |
            <a href="mailto:zhenfei.yin@sydney.edu.au">zhenfei.yin@sydney.edu.au</a>
          </h5>
    
        </div>

      </div>

    </section><!-- End Contact Section -->

    <!-- ======= Contact Section ======= -->
    <!-- <section id="sponsors" class="sponsors">
      <div class="container">

        <div class="section-title" data-aos="zoom-out">
          <h2>Sponsors</h2>
        </div>

        <p>
          <a href="https://www.multion.ai/">MultiOn AI</a>
        </p>
        <br>

        <div class="member-img">
          <img src="assets/img/multion_logo.jpeg" class="img-fluid" alt>
        </div>

        <br><br>
        <p>
          <a href="https://www.occam.ai/">Occam AI</a>

        </p>


        <br>
        <div class="member-img">
          <img src="assets/img/Occam_Ai_black_logo_2 _transparent.png" class="img-fluid" alt>
        </div>



        <br><br>
        <p>
          <a href="https://www.orby.ai/">Orby AI</a>

        </p>


        <br>
        <div class="member-img">
          <img src="assets/img/Orby AI-2.jpg" class="img-fluid" alt>
        </div>




      </div> -->


  </main><!-- End #main -->

  <!-- ======= Footer ======= -->
  <footer id="footer">
    <div class="container">
      <div class="copyright">
        &copy; Copyright <strong><strong>Selecao</strong></strong>.
        All Rights Reserved
      </div>
      <div class="credits">
        <!-- All the links in the footer should remain intact. -->
        <!-- You can delete the links only if you purchased the pro version. -->
        <!-- Licensing information: https://bootstrapmade.com/license/ -->
        <!-- Purchase the pro version with working PHP/AJAX contact form: https://bootstrapmade.com/selecao-bootstrap-template/ -->
        Designed by <a href="https://bootstrapmade.com/">BootstrapMade</a>
        and <a href="https://doc2dial.github.io/workshop2022/">DialDoc</a>
      </div>
    </div>
  </footer><!-- End Footer -->

  <a href="#" class="back-to-top d-flex align-items-center justify-content-center"><i
      class="bi bi-arrow-up-short"></i></a>

  <!-- Vendor JS Files -->
  <script src="assets/vendor/aos/aos.js"></script>
  <script src="assets/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
  <script src="assets/vendor/glightbox/js/glightbox.min.js"></script>
  <script src="assets/vendor/isotope-layout/isotope.pkgd.min.js"></script>
  <script src="assets/vendor/php-email-form/validate.js"></script>
  <script src="assets/vendor/swiper/swiper-bundle.min.js"></script>

  <!-- Template Main JS File -->
  <script src="assets/js/main.js"></script>

  <!-- Default Statcounter code for llmagents.github.io
https://llmagents.github.io/ -->
  <script type="text/javascript">
    var sc_project = 12953394;
    var sc_invisible = 1;
    var sc_security = "86f01d45"; 
  </script>
  <script type="text/javascript" src="https://www.statcounter.com/counter/counter.js" async></script>
  <noscript>
    <div class="statcounter"><a title="Web Analytics" href="https://statcounter.com/" target="_blank"><img
          class="statcounter" src="https://c.statcounter.com/12953394/0/86f01d45/1/" alt="Web Analytics"
          referrerPolicy="no-referrer-when-downgrade"></a></div>
  </noscript>
  <!-- End of Statcounter Code -->

</body>

</html>